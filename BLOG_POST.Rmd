---
title: "Predicting Hospital Readmissions: A Machine Learning Journey"
subtitle: "How I Built Three Models to Predict 30-Day Readmissions in Diabetes Patients"
author: "Masheia Dzimba and Peter Mangoro"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
    fig_width: 10
    fig_height: 6
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, 
                      fig.align = "center", fig.width = 10, fig.height = 6,
                      fig.path = "blog_assets/images/")

# Load libraries
library(ggplot2)
library(dplyr)
library(knitr)
library(kableExtra)
library(pROC)
library(caret)
library(rpart)
library(randomForest)

# Load data and results
data_clean <- read.csv("data_clean.csv", stringsAsFactors = FALSE)
metrics_lr <- read.csv("plots/04_performance_metrics.csv")
metrics_cart <- read.csv("plots/05_performance_metrics.csv")
metrics_rf <- read.csv("plots/05b_performance_metrics.csv")
auc_lr <- read.csv("plots/04_auc.csv")
auc_cart <- read.csv("plots/05_auc.csv")
auc_rf <- read.csv("plots/05b_auc.csv")
reg_output <- read.csv("plots/04_regression_output.csv")
var_imp_cart <- read.csv("plots/05_variable_importance.csv")
var_imp_rf <- read.csv("plots/05b_variable_importance.csv")
comp_table <- read.csv("plots/06_model_comparison.csv")

# Calculate key statistics
n_total <- nrow(data_clean)
n_readmitted <- sum(data_clean$readmitted_binary == 1)
n_not_readmitted <- sum(data_clean$readmitted_binary == 0)
pct_readmitted <- round((n_readmitted / n_total) * 100, 2)
pct_not_readmitted <- round((n_not_readmitted / n_total) * 100, 2)

# Extract metrics
lr_accuracy <- round(metrics_lr$Percentage[metrics_lr$Metric == "Accuracy"], 2)
lr_auc <- round(auc_lr$Value, 3)
cart_accuracy <- round(metrics_cart$Percentage[metrics_cart$Metric == "Accuracy"], 2)
cart_auc <- round(auc_cart$Value, 3)
rf_accuracy <- round(metrics_rf$Percentage[metrics_rf$Metric == "Accuracy"], 2)
rf_auc <- round(auc_rf$Value, 3)

# Top predictors
top_predictor_lr <- reg_output$Variable[which.max(reg_output$Odds_Ratio)]
top_predictor_lr_or <- round(reg_output$Odds_Ratio[which.max(reg_output$Odds_Ratio)], 2)
top_predictor_cart <- var_imp_cart$Variable[1]
top_predictor_cart_imp <- round(var_imp_cart$Importance_Percent[1], 2)
top_predictor_rf <- var_imp_rf$Variable[1]
top_predictor_rf_imp <- round(var_imp_rf$Importance_Percent[1], 2)
```

# The Problem: Why Hospital Readmissions Matter

Imagine you're a hospital administrator. Every day, you see patients being discharged, and you wonder: *"Will this patient be back within 30 days?"* 

Hospital readmissions are a **$15 billion problem** in the United States. For patients with diabetes, the stakes are even higherâ€”they're more likely to be readmitted, which means:

- **Higher costs** for healthcare systems
- **Worse outcomes** for patients  
- **Strain on resources** that could be better allocated

But what if we could predict which patients are at high risk of readmission? That's exactly what I set out to do in this project.

# The Data: 25,000 Patient Stories

I worked with a dataset of **`r n_total` patient encounters** from 130 US hospitals over 10 years (1999-2008). Each row tells a story:

- How long did they stay in the hospital?
- How many medications were they on?
- What was their primary diagnosis?
- Had they been to the hospital before?

The dataset had a **`r pct_readmitted`% readmission rate**â€”nearly half of all patients came back within 30 days. This is a significant problem that needs solving.

```{r readmission-viz, echo=FALSE, fig.cap="Distribution of Readmitted vs. Not Readmitted Patients", out.width="80%"}
# Create a clean visualization for the blog
readmission_data <- data.frame(
  Status = c("Readmitted", "Not Readmitted"),
  Count = c(n_readmitted, n_not_readmitted),
  Percentage = c(pct_readmitted, pct_not_readmitted)
)

p_readmission <- ggplot(readmission_data, aes(x = Status, y = Count, fill = Status)) +
  geom_bar(stat = "identity", alpha = 0.8) +
  scale_fill_manual(values = c("Readmitted" = "#e74c3c", "Not Readmitted" = "#3498db")) +
  geom_text(aes(label = paste0(Count, "\n(", Percentage, "%)")), 
            vjust = -0.5, size = 5, fontface = "bold") +
  labs(title = "Distribution of Readmitted vs. Not Readmitted Patients",
       subtitle = paste("Total:", n_total, "patient encounters"),
       x = "", y = "Number of Patients") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        legend.position = "none",
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 12))

print(p_readmission)
```

# My Approach: Three Models, One Goal

I decided to build **three different machine learning models** to see which approach worked best:

1. **Logistic Regression** - The classic statistical approach, great for interpretability
2. **CART (Decision Trees)** - Simple, visual, easy to explain
3. **Random Forest** - An ensemble method that combines many trees

Each model has its strengths, and I wanted to see which one would give us the best predictions.

## Model 1: Logistic Regression

Logistic Regression was my starting point. It's interpretable and provides odds ratios that clinicians can understand.

**Key Insight**: The model showed that **`r top_predictor_lr`** (OR: `r top_predictor_lr_or`) was the strongest predictor of readmission. Patients with more previous visits had significantly higher odds of being readmitted.

```{r lr-results, echo=FALSE, results='asis'}
cat("**Logistic Regression Performance:**\n\n")
cat("- **Accuracy**: ", lr_accuracy, "%\n")
cat("- **AUC-ROC**: ", lr_auc, "\n")
cat("- **Interpretability**: â­â­â­â­â­ (Excellent - provides odds ratios and p-values)\n")
```

```{r lr-roc, echo=FALSE, fig.cap="ROC Curve: Logistic Regression Model", out.width="70%"}
# Load and display ROC curve
if(file.exists("plots/04_roc_curve_ggplot.png")) {
  knitr::include_graphics("plots/04_roc_curve_ggplot.png")
} else {
  # Generate ROC curve if image doesn't exist
  load("model_logistic.RData")
  load("data_logistic.RData")
  
  set.seed(1)
  train_indices <- createDataPartition(data_logistic$readmitted, p = 0.7, list = FALSE)
  data_test_lr <- data_logistic[-train_indices, ]
  
  predictions_prob <- predict(model_logistic, newdata = data_test_lr, type = "response")
  roc_obj <- roc(data_test_lr$readmitted, predictions_prob)
  
  roc_data <- data.frame(
    FPR = 1 - roc_obj$specificities,
    TPR = roc_obj$sensitivities
  )
  
  p_roc <- ggplot(roc_data, aes(x = FPR, y = TPR)) +
    geom_line(color = "steelblue", linewidth = 1.2) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", linewidth = 1) +
    labs(title = "ROC Curve: Logistic Regression Model",
         subtitle = paste("AUC =", round(auc(roc_obj), 3)),
         x = "False Positive Rate (1 - Specificity)",
         y = "True Positive Rate (Sensitivity)") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
          plot.subtitle = element_text(hjust = 0.5, size = 12))
  
  print(p_roc)
}
```

## Model 2: CART (Decision Trees)

Decision trees are like a flowchartâ€”they ask yes/no questions to classify patients. I loved how visual and intuitive this approach was.

The tree showed that **`r top_predictor_cart`** (`r top_predictor_cart_imp`% importance) was the most important factor, splitting patients into high-risk and low-risk groups.

```{r cart-results, echo=FALSE, results='asis'}
cat("**CART Performance:**\n\n")
cat("- **Accuracy**: ", cart_accuracy, "%\n")
cat("- **AUC-ROC**: ", cart_auc, "\n")
cat("- **Interpretability**: â­â­â­â­ (Great - visual decision rules)\n")
```

```{r cart-tree, echo=FALSE, fig.cap="CART Decision Tree Visualization", out.width="80%"}
if(file.exists("plots/05_cart_tree_compact.png")) {
  knitr::include_graphics("plots/05_cart_tree_compact.png")
}
```

## Model 3: Random Forest

Random Forest combines hundreds of decision trees, each trained on a different subset of the data. It's like asking a committee of experts instead of just one.

```{r rf-results, echo=FALSE, results='asis'}
cat("**Random Forest Performance:**\n\n")
cat("- **Accuracy**: ", rf_accuracy, "%\n")
cat("- **AUC-ROC**: ", rf_auc, "\n")
cat("- **Interpretability**: â­â­â­ (Good - shows feature importance)\n")
```

```{r rf-importance, echo=FALSE, fig.cap="Random Forest: Top 10 Most Important Variables", out.width="80%"}
# Create feature importance plot
top10_rf <- head(var_imp_rf, 10)

p_rf_imp <- ggplot(top10_rf, aes(x = reorder(Variable, Importance_Percent), 
                                  y = Importance_Percent)) +
  geom_bar(stat = "identity", fill = "darkgreen", alpha = 0.7) +
  coord_flip() +
  labs(title = "Random Forest: Top 10 Most Important Variables",
       x = "Variable",
       y = "Importance (%)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))

print(p_rf_imp)
```

# The Results: What We Learned

## Performance Comparison

```{r comparison-table, echo=FALSE, results='asis'}
comparison_df <- data.frame(
  Model = c("Logistic Regression", "CART", "Random Forest"),
  Accuracy = paste0(c(lr_accuracy, cart_accuracy, rf_accuracy), "%"),
  AUC = c(lr_auc, cart_auc, rf_auc),
  Interpretability = c("â­â­â­â­â­", "â­â­â­â­", "â­â­â­")
)

kable(comparison_df, caption = "Model Performance Comparison", align = "lcc") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = FALSE, position = "center")
```

## Key Findings

1. **All three models performed similarly** (~60-62% accuracy), suggesting the problem is inherently challenging with the available features.

2. **Previous hospital visits** consistently emerged as the strongest predictor across all models. This makes intuitive senseâ€”patients with complex medical histories are more likely to need readmission.

3. **Number of diagnoses** was another important factor. Patients with multiple conditions are at higher risk.

4. **Medical specialty** mattered too. Patients seen in Emergency/Trauma departments had higher readmission rates.

## ROC Curves: Visualizing Model Performance

```{r roc-comparison, echo=FALSE, fig.cap="ROC Curves Comparing All Three Models", out.width="80%"}
if(file.exists("plots/06_auc_comparison.png")) {
  knitr::include_graphics("plots/06_auc_comparison.png")
} else {
  # Generate comparison if image doesn't exist
  # This would require loading all models and test data
  cat("ROC comparison plot would be generated here")
}
```

# What I Learned: Challenges and Insights

## The Challenge of ~60% Accuracy

At first, I was disappointed with ~60% accuracy. But then I realized:

- **This is a hard problem**. Even experienced clinicians struggle to predict readmissions.
- **The dataset has limitations**. We're missing important clinical variables like lab values, vital signs, and social determinants of health.
- **60% is a starting point**. With better features and more data, we could improve significantly.

## What Would Improve the Model?

If I had access to more data, I would add:

1. **Lab values**: Blood glucose, HbA1c, creatinine, etc.
2. **Vital signs**: Blood pressure, heart rate, temperature
3. **Social determinants**: Insurance type, socioeconomic status, housing stability
4. **Medication adherence**: Are patients taking their medications as prescribed?
5. **Follow-up care**: Did patients attend follow-up appointments?

## The Power of Interpretability

One of my biggest takeaways was the importance of **interpretability** in healthcare. Clinicians need to understand *why* a model makes a prediction, not just that it does. That's why Logistic Regression, despite similar performance, might be more useful in practiceâ€”it provides odds ratios and statistical significance that doctors can interpret.

# Building an Interactive Dashboard

To make this project more practical, I created a **Shiny web application** where users can input patient information and get real-time readmission risk predictions from all three models.

The app allows healthcare providers to:
- Input patient demographics and medical history
- See predictions from all three models
- Compare model probabilities
- View feature importance and ROC curves

# Technical Stack

For those interested in the technical details:

- **Language**: R
- **Libraries**: `caret`, `rpart`, `randomForest`, `pROC`, `ggplot2`, `dplyr`
- **Visualization**: `ggplot2`, `rpart.plot`
- **Interactive App**: R Shiny
- **Report Generation**: R Markdown

# Conclusion: What's Next?

This project taught me that **predicting healthcare outcomes is complex**, but machine learning can provide valuable insights. While 60% accuracy might not seem impressive, it's a solid foundation that could be improved with:

- Better feature engineering
- More clinical variables
- Advanced techniques like gradient boosting
- Ensemble methods combining all three models

Most importantly, I learned that in healthcare, **interpretability matters just as much as accuracy**. A model that doctors can understand and trust is often more valuable than a black box with slightly better performance.

---

## Try It Yourself

- **ðŸ“Š [View the Full Report](PROJECT_REPORT.pdf)** - Detailed technical analysis
- **ðŸ’» [GitHub Repository](https://github.com/yourusername/hospital-readmissions)** - Full code and data
- **ðŸš€ [Interactive Dashboard](https://your-app-url.shinyapps.io/hospital-readmissions/)** - Try the models yourself

---

*This project was completed as part of a Computational Mathematics course. All code and analysis are available on GitHub.*

